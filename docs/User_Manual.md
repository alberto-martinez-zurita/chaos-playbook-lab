# üìö Manual de Usuario - Chaos Playbook Engine

**Versi√≥n:** 2.0  
**Fecha:** 24 Noviembre 2025  
**Autor:** Albert - Stanford Google Cloud AI Capstone

---

## üìã √çndice

1. [Introducci√≥n](#introducci√≥n)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
4. [Comandos Principales](#comandos-principales)
5. [M√≥dulos y Componentes](#m√≥dulos-y-componentes)
6. [Experimentos Param√©tricos](#experimentos-param√©tricos)
7. [Generaci√≥n de Reportes](#generaci√≥n-de-reportes)
8. [Tests y Cobertura](#tests-y-cobertura)
9. [Configuraci√≥n Avanzada](#configuraci√≥n-avanzada)
10. [Troubleshooting](#troubleshooting)
11. [Ap√©ndices](#ap√©ndices)

---

## üéØ Introducci√≥n

### ¬øQu√© es Chaos Playbook Engine?

Chaos Playbook Engine es un framework de experimentaci√≥n para agentes AI que permite:

- **Inyecci√≥n controlada de fallos** en APIs simuladas
- **Comparaci√≥n A/B** entre agentes (Baseline vs Playbook)
- **Experimentos param√©tricos** con m√∫ltiples failure rates
- **Generaci√≥n autom√°tica de m√©tricas** y dashboards interactivos
- **Validaci√≥n de resilencia** de agentes AI

### Prop√≥sito

Validar que los agentes AI equipados con "playbooks" (estrategias de recuperaci√≥n de errores) son m√°s efectivos que agentes baseline sin estrategias.

### Caso de Uso Principal

Sistema de pedidos de comida con:
- **APIs simuladas**: Inventory, Payment, Shipping
- **Fallos controlados**: Timeouts, errores HTTP, inconsistencias
- **M√©tricas**: Success rate, latency, data consistency

---

## üèóÔ∏è Arquitectura del Sistema

### Estructura de Directorios

```
chaos-playbook-engine/
‚îú‚îÄ‚îÄ src/                          # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # Componentes core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ab_test_runner.py    # Runner de experimentos A/B
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aggregate_metrics.py # Agregaci√≥n de m√©tricas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chaos_config.py      # Configuraci√≥n de chaos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chaos_injection_helper.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retry_wrapper.py     # Wrapper de reintentos
‚îÇ   ‚îú‚îÄ‚îÄ agents/                   # Agentes AI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ order_orchestrator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ experiment_judge.py
‚îÇ   ‚îú‚îÄ‚îÄ storage/                  # Persistencia
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ playbook_storage.py
‚îÇ   ‚îî‚îÄ‚îÄ apis/                     # APIs simuladas
‚îÇ       ‚îî‚îÄ‚îÄ simulated_apis.py
‚îú‚îÄ‚îÄ tests/                        # Tests unitarios
‚îÇ   ‚îú‚îÄ‚îÄ test_ab_runner.py
‚îÇ   ‚îú‚îÄ‚îÄ test_aggregate_metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ test_chaos_injection.py
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py
‚îú‚îÄ‚îÄ scripts/                      # Scripts de utilidad
‚îÇ   ‚îú‚îÄ‚îÄ run_parametric_experiments.py
‚îÇ   ‚îî‚îÄ‚îÄ generate_dashboard.py
‚îú‚îÄ‚îÄ playbooks/                    # Playbooks JSON
‚îÇ   ‚îî‚îÄ‚îÄ chaos_playbook.json
‚îú‚îÄ‚îÄ results/                      # Resultados de experimentos
‚îÇ   ‚îî‚îÄ‚îÄ parametric_experiments/
‚îÇ       ‚îî‚îÄ‚îÄ run_YYYYMMDD_HHMMSS/
‚îÇ           ‚îú‚îÄ‚îÄ raw_results.csv
‚îÇ           ‚îú‚îÄ‚îÄ aggregated_metrics.json
‚îÇ           ‚îî‚îÄ‚îÄ dashboard.html
‚îú‚îÄ‚îÄ pyproject.toml                # Dependencias Poetry
‚îî‚îÄ‚îÄ README.md
```

### Flujo de Datos

```
1. Configuraci√≥n
   ‚îú‚îÄ> chaos_config.py (failure rates, scenarios)
   ‚îî‚îÄ> chaos_playbook.json (estrategias de recuperaci√≥n)

2. Ejecuci√≥n
   ‚îú‚îÄ> run_parametric_experiments.py
   ‚îÇ   ‚îú‚îÄ> ab_test_runner.py (ejecuta experimentos)
   ‚îÇ   ‚îÇ   ‚îú‚îÄ> order_orchestrator.py (baseline/playbook)
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ> simulated_apis.py (APIs con fallos)
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ> chaos_injection_helper.py (inyecta fallos)
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> experiment_judge.py (valida resultados)
   ‚îÇ   ‚îî‚îÄ> aggregate_metrics.py (agrega m√©tricas)

3. An√°lisis
   ‚îú‚îÄ> generate_dashboard.py (genera HTML)
   ‚îî‚îÄ> Dashboard interactivo (Plotly)
```

---

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n

### Requisitos Previos

- **Python**: 3.10+
- **Poetry**: 1.5+
- **Google Cloud SDK** (opcional, para Vertex AI)

### Instalaci√≥n Inicial

```powershell
# 1. Clonar repositorio
git clone https://github.com/your-repo/chaos-playbook-engine.git
cd chaos-playbook-engine

# 2. Instalar dependencias con Poetry
poetry install

# 3. Activar entorno virtual
poetry shell

# 4. Verificar instalaci√≥n
poetry run python --version
poetry run pytest --version
```

### Configuraci√≥n de Variables de Entorno

Crear archivo `.env`:

```bash
# Google Cloud
GOOGLE_CLOUD_PROJECT=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# Vertex AI
VERTEX_AI_LOCATION=us-central1
VERTEX_AI_MODEL=gemini-1.5-flash-002

# Chaos Configuration
DEFAULT_FAILURE_RATE=0.3
CHAOS_SEED=42
```

### Verificaci√≥n de Instalaci√≥n

```powershell
# Ejecutar tests r√°pidos
poetry run pytest tests/ -v --tb=short

# Verificar m√≥dulos importables
poetry run python -c "from src.core.ab_test_runner import ABTestRunner; print('OK')"
```

---

## üöÄ Comandos Principales

### 1. Ejecutar Experimento Param√©trico

**Comando b√°sico:**

```powershell
poetry run python scripts/run_parametric_experiments.py
```

**Con opciones:**

```powershell
# Especificar failure rates personalizados
poetry run python scripts/run_parametric_experiments.py \
    --failure-rates 0.1 0.3 0.5

# N√∫mero de experimentos por configuraci√≥n
poetry run python scripts/run_parametric_experiments.py \
    --n-experiments 5

# Timeout personalizado
poetry run python scripts/run_parametric_experiments.py \
    --timeout 120

# Verbose mode
poetry run python scripts/run_parametric_experiments.py --verbose
```

**Opciones disponibles:**

| Opci√≥n | Descripci√≥n | Valor por defecto |
|--------|-------------|-------------------|
| `--failure-rates` | Lista de failure rates (0.0-1.0) | `[0.1, 0.3, 0.5]` |
| `--n-experiments` | N√∫mero de experimentos por rate | `2` |
| `--timeout` | Timeout en segundos | `60` |
| `--output-dir` | Directorio de salida | `results/parametric_experiments` |
| `--verbose` | Modo detallado | `False` |
| `--seed` | Semilla para reproducibilidad | `42` |

**Salida esperada:**

```
üéØ Starting Parametric Experiment Suite
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìä Configuration:
   ‚Ä¢ Failure Rates: [0.1, 0.3, 0.5]
   ‚Ä¢ Experiments per rate: 2
   ‚Ä¢ Total experiments: 12 (6 baseline + 6 playbook)

üî¨ Running Experiments...
   ‚úì Failure Rate 10.0% - Baseline [1/2] ... DONE (2.34s)
   ‚úì Failure Rate 10.0% - Baseline [2/2] ... DONE (2.41s)
   ...

üìà Aggregating Metrics...
   ‚úì Aggregated metrics saved to: results/.../aggregated_metrics.json

‚úÖ Experiment Suite Complete!
   üìÅ Results: results/parametric_experiments/run_20251124_003045/
```

### 2. Generar Dashboard HTML

**Comando b√°sico:**

```powershell
poetry run python scripts/generate_dashboard.py --latest
```

**Con opciones:**

```powershell
# Dashboard de un run espec√≠fico
poetry run python scripts/generate_dashboard.py \
    --run-dir run_20251124_003045

# Output personalizado
poetry run python scripts/generate_dashboard.py \
    --latest \
    --output custom_dashboard.html

# Abrir autom√°ticamente en navegador (Windows)
poetry run python scripts/generate_dashboard.py --latest
start results/parametric_experiments/run_XXXXX/dashboard.html
```

**Opciones disponibles:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--latest` | Usar el run m√°s reciente |
| `--run-dir` | Especificar run directory name |
| `--output` | Path personalizado para dashboard |

**Salida esperada:**

```
üé® Generating dashboard from: results/.../aggregated_metrics.json
   Output: results/.../dashboard.html

‚úÖ Dashboard generated successfully!
   Location: results/parametric_experiments/run_20251124_003045/dashboard.html
   Size: 124.5 KB

üåê Open in browser: file:///path/to/dashboard.html
```

### 3. Ejecutar Tests

**Tests completos:**

```powershell
poetry run pytest tests/ -v
```

**Tests espec√≠ficos:**

```powershell
# Test de un m√≥dulo espec√≠fico
poetry run pytest tests/test_ab_runner.py -v

# Test de una funci√≥n espec√≠fica
poetry run pytest tests/test_ab_runner.py::test_run_experiment -v

# Tests con markers
poetry run pytest -m "not slow" -v
```

**Con cobertura de c√≥digo:**

```powershell
# Cobertura b√°sica
poetry run pytest tests/ --cov=src --cov-report=term-missing

# Cobertura con HTML
poetry run pytest tests/ --cov=src --cov-report=html
start htmlcov/index.html

# Con umbral m√≠nimo (falla si < 80%)
poetry run pytest tests/ --cov=src --cov-fail-under=80
```

**Opciones de pytest:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `-v` | Verbose mode |
| `-s` | Mostrar prints |
| `-x` | Parar en primer fallo |
| `--tb=short` | Traceback corto |
| `--lf` | Ejecutar solo √∫ltimos fallidos |
| `--ff` | Ejecutar fallidos primero |
| `-k "pattern"` | Filtrar por nombre de test |

### 4. Linting y Formateo

```powershell
# Black (formateador)
poetry run black src/ tests/

# Flake8 (linter)
poetry run flake8 src/ tests/

# MyPy (type checker)
poetry run mypy src/

# Todo en uno
poetry run black src/ tests/ && \
poetry run flake8 src/ tests/ && \
poetry run mypy src/
```

### 5. Gesti√≥n de Dependencias

```powershell
# Ver dependencias instaladas
poetry show

# Agregar dependencia
poetry add requests

# Agregar dependencia de desarrollo
poetry add --group dev pytest-mock

# Actualizar dependencias
poetry update

# Exportar requirements.txt
poetry export -f requirements.txt --output requirements.txt
```

---

## üß© M√≥dulos y Componentes

### 1. ABTestRunner (`ab_test_runner.py`)

**Prop√≥sito:** Ejecutor principal de experimentos A/B.

**Uso:**

```python
from src.core.ab_test_runner import ABTestRunner

runner = ABTestRunner(
    failure_rate=0.3,
    n_experiments=5,
    timeout=60,
    use_playbook=True
)

results = runner.run_experiment()
```

**M√©todos principales:**

| M√©todo | Descripci√≥n | Par√°metros |
|--------|-------------|------------|
| `run_experiment()` | Ejecuta experimento completo | `None` |
| `_run_single_experiment()` | Ejecuta 1 experimento | `experiment_id: int` |
| `_compare_agents()` | Compara baseline vs playbook | `None` |

**Salida:**

```python
{
    "failure_rate": 0.3,
    "n_experiments": 5,
    "baseline": {
        "success_rate": 0.6,
        "avg_duration": 2.34,
        "inconsistencies": 0.2
    },
    "playbook": {
        "success_rate": 0.8,
        "avg_duration": 2.51,
        "inconsistencies": 0.1
    },
    "raw_results": [...]
}
```

### 2. AggregateMetrics (`aggregate_metrics.py`)

**Prop√≥sito:** Agregaci√≥n y an√°lisis estad√≠stico de resultados.

**Uso:**

```python
from src.core.aggregate_metrics import aggregate_results

aggregated = aggregate_results(
    csv_path="results/run_XXX/raw_results.csv",
    output_path="results/run_XXX/aggregated_metrics.json"
)
```

**M√©tricas calculadas:**

- **Success Rate**: mean, std, min, max
- **Duration**: mean, std, min, max
- **Inconsistencies**: mean, std, min, max
- **Agent Performance**: baseline vs playbook deltas

### 3. ChaosConfig (`chaos_config.py`)

**Prop√≥sito:** Configuraci√≥n de escenarios de chaos.

**Estructura:**

```python
CHAOS_SCENARIOS = {
    "inventory_timeout": {
        "api": "inventory",
        "failure_type": "timeout",
        "probability": 0.3,
        "delay_ms": 5000
    },
    "payment_http_error": {
        "api": "payment",
        "failure_type": "http_error",
        "probability": 0.2,
        "status_code": 503
    },
    # ... m√°s escenarios
}
```

**Tipos de fallos soportados:**

| Tipo | Descripci√≥n | Par√°metros |
|------|-------------|------------|
| `timeout` | API no responde | `delay_ms` |
| `http_error` | Error HTTP | `status_code` |
| `data_corruption` | Datos corruptos | `corruption_type` |
| `partial_failure` | Respuesta parcial | `missing_fields` |

### 4. SimulatedAPIs (`simulated_apis.py`)

**Prop√≥sito:** APIs simuladas con inyecci√≥n de fallos.

**APIs disponibles:**

```python
# Inventory API
response = inventory_api.check_inventory(order_id)
# Returns: {"items": [...], "available": true/false}

# Payment API
response = payment_api.process_payment(order_id, amount)
# Returns: {"transaction_id": "...", "status": "success"}

# Shipping API
response = shipping_api.schedule_shipping(order_id, address)
# Returns: {"tracking_id": "...", "eta": "2025-11-25"}
```

**Configuraci√≥n de fallos:**

```python
from src.apis.simulated_apis import InventoryAPI

api = InventoryAPI(failure_rate=0.3, chaos_enabled=True)
response = api.check_inventory(order_id="ORD123")
```

### 5. OrderOrchestrator (`order_orchestrator.py`)

**Prop√≥sito:** Agente orquestador de pedidos.

**Modos:**

- **Baseline**: Sin estrategias de recuperaci√≥n
- **Playbook**: Con estrategias de recuperaci√≥n

**Uso:**

```python
from src.agents.order_orchestrator import OrderOrchestrator

# Baseline agent
orchestrator = OrderOrchestrator(use_playbook=False)
result = orchestrator.process_order(order_data)

# Playbook agent
orchestrator = OrderOrchestrator(use_playbook=True)
result = orchestrator.process_order(order_data)
```

**Estrategias de playbook:**

- **Retry con exponential backoff**
- **Circuit breaker**
- **Fallback a valores por defecto**
- **Compensating transactions**

---

## üî¨ Experimentos Param√©tricos

### Configuraci√≥n de Experimentos

**Archivo:** `scripts/run_parametric_experiments.py`

**Par√°metros configurables:**

```python
FAILURE_RATES = [0.1, 0.2, 0.3, 0.4, 0.5]  # Lista de failure rates
N_EXPERIMENTS = 5                           # Experimentos por rate
TIMEOUT = 60                                # Timeout en segundos
OUTPUT_DIR = "results/parametric_experiments"
```

### Flujo de Ejecuci√≥n

```
1. Setup
   ‚îú‚îÄ> Crear directorio de resultados
   ‚îú‚îÄ> Inicializar logger
   ‚îî‚îÄ> Cargar configuraci√≥n

2. Para cada failure rate:
   ‚îú‚îÄ> Ejecutar N experimentos baseline
   ‚îú‚îÄ> Ejecutar N experimentos playbook
   ‚îî‚îÄ> Guardar raw_results.csv

3. Agregaci√≥n
   ‚îú‚îÄ> Calcular estad√≠sticas por failure rate
   ‚îú‚îÄ> Calcular deltas (playbook - baseline)
   ‚îî‚îÄ> Guardar aggregated_metrics.json

4. Dashboard
   ‚îî‚îÄ> Generar dashboard.html interactivo
```

### M√©tricas Capturadas

**Por experimento individual:**

```json
{
  "experiment_id": 1,
  "agent_type": "baseline",
  "failure_rate": 0.3,
  "success": true,
  "duration_s": 2.34,
  "inconsistencies": 0,
  "api_calls": {
    "inventory": {"success": true, "duration": 0.5},
    "payment": {"success": true, "duration": 0.8},
    "shipping": {"success": false, "duration": 1.0}
  }
}
```

**M√©tricas agregadas:**

```json
{
  "0.3": {
    "failure_rate": 0.3,
    "n_experiments": 5,
    "baseline": {
      "success_rate": {"mean": 0.6, "std": 0.1, "min": 0.4, "max": 0.8},
      "duration_s": {"mean": 2.34, "std": 0.5, "min": 1.8, "max": 3.0},
      "inconsistencies": {"mean": 0.2, "std": 0.1, "min": 0.0, "max": 0.4}
    },
    "playbook": {
      "success_rate": {"mean": 0.8, "std": 0.05, "min": 0.7, "max": 0.9},
      "duration_s": {"mean": 2.51, "std": 0.4, "min": 2.0, "max": 3.1},
      "inconsistencies": {"mean": 0.1, "std": 0.05, "min": 0.0, "max": 0.2}
    }
  }
}
```

---

## üìä Generaci√≥n de Reportes

### Dashboard Interactivo

**Archivo:** `scripts/generate_dashboard.py`

**Componentes del dashboard:**

1. **Executive Summary Cards**
   - Max Effectiveness Gain
   - Avg Latency (Baseline)
   - Avg Latency (Playbook)
   - Avg Consistency (Playbook)

2. **Gr√°ficos Interactivos (Plotly)**
   - Agent Effectiveness (bar chart)
   - Latency Overhead % (line chart)
   - Data Consistency % (line chart)
   - Combined Performance Trends (multi-line)

3. **Summary Results Tables**
   - Success Rate Summary (por failure rate)
   - Latency Overhead Rate Summary
   - Consistency Rate Summary

4. **Detailed Results Tables**
   - M√©tricas detalladas por failure rate
   - Comparaci√≥n baseline vs playbook
   - Deltas con color coding (verde/rojo/negro)

**Caracter√≠sticas interactivas:**

- **Zoom/Pan** en gr√°ficos
- **Hover tooltips** con valores exactos
- **Exportar gr√°ficos** (PNG, SVG)
- **Responsive design** (mobile-friendly)

### Comando de Generaci√≥n

```powershell
# Dashboard del run m√°s reciente
poetry run python scripts/generate_dashboard.py --latest

# Dashboard de run espec√≠fico
poetry run python scripts/generate_dashboard.py \
    --run-dir run_20251124_003045

# Con output personalizado
poetry run python scripts/generate_dashboard.py \
    --latest \
    --output /path/to/custom_dashboard.html
```

### Estructura HTML Generada

```html
<!DOCTYPE html>
<html>
<head>
    <title>Chaos Playbook Engine - Dashboard</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <style>/* Estilos responsivos */</style>
</head>
<body>
    <div class="container">
        <div class="header">...</div>
        <div class="metadata">...</div>
        <div class="content">
            <!-- Executive Summary -->
            <div class="summary-cards">...</div>
            
            <!-- Charts -->
            <div class="charts-grid">...</div>
            
            <!-- Tables -->
            <div class="summary-tables">...</div>
            <div class="detailed-tables">...</div>
        </div>
        <div class="footer">...</div>
    </div>
    <script>/* Plotly charts */</script>
</body>
</html>
```

---

## üß™ Tests y Cobertura

### Estructura de Tests

```
tests/
‚îú‚îÄ‚îÄ conftest.py                    # Fixtures compartidos
‚îú‚îÄ‚îÄ test_ab_runner.py              # Tests de ABTestRunner
‚îú‚îÄ‚îÄ test_aggregate_metrics.py      # Tests de agregaci√≥n
‚îú‚îÄ‚îÄ test_chaos_injection.py        # Tests de chaos injection
‚îú‚îÄ‚îÄ test_order_orchestrator.py     # Tests de orchestrator
‚îú‚îÄ‚îÄ test_experiment_judge.py       # Tests de judge
‚îú‚îÄ‚îÄ test_simulated_apis.py         # Tests de APIs
‚îî‚îÄ‚îÄ test_playbook_storage.py       # Tests de storage
```

### Fixtures Principales (conftest.py)

```python
@pytest.fixture
def sample_order_data():
    """Datos de orden de prueba"""
    return {
        "order_id": "TEST-001",
        "items": [...],
        "total": 150.00
    }

@pytest.fixture
def mock_apis(monkeypatch):
    """Mock de APIs simuladas"""
    # ... mocking setup

@pytest.fixture
def temp_results_dir(tmp_path):
    """Directorio temporal para resultados"""
    return tmp_path / "test_results"
```

### Ejecutar Tests

**Tests completos:**

```powershell
poetry run pytest tests/ -v
```

**Tests con cobertura:**

```powershell
# Terminal output
poetry run pytest tests/ --cov=src --cov-report=term-missing

# HTML report
poetry run pytest tests/ --cov=src --cov-report=html
start htmlcov/index.html

# XML (para CI/CD)
poetry run pytest tests/ --cov=src --cov-report=xml
```

**Tests espec√≠ficos:**

```powershell
# Un archivo
poetry run pytest tests/test_ab_runner.py -v

# Una clase
poetry run pytest tests/test_ab_runner.py::TestABTestRunner -v

# Una funci√≥n
poetry run pytest tests/test_ab_runner.py::test_run_experiment -v

# Por marker
poetry run pytest -m "not slow" -v
```

### Cobertura de C√≥digo

**Objetivo:** >80% coverage

**Configuraci√≥n en pyproject.toml:**

```toml
[tool.pytest.ini_options]
addopts = "--cov=src --cov-report=term-missing --cov-report=html"
testpaths = ["tests"]

[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/__pycache__/*"
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if __name__ == .__main__.:"
]
```

**Ver reporte de cobertura:**

```powershell
# Ejecutar tests con cobertura
poetry run pytest tests/ --cov=src --cov-report=html

# Abrir reporte HTML
start htmlcov/index.html
```

**M√©tricas de cobertura:**

| M√≥dulo | Statements | Missing | Coverage |
|--------|-----------|---------|----------|
| ab_test_runner.py | 145 | 12 | 91% |
| aggregate_metrics.py | 98 | 5 | 94% |
| chaos_config.py | 42 | 2 | 95% |
| order_orchestrator.py | 203 | 18 | 91% |
| **TOTAL** | **488** | **37** | **92%** |

---

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Chaos Playbook JSON

**Archivo:** `playbooks/chaos_playbook.json`

**Estructura:**

```json
{
  "version": "2.0",
  "scenarios": {
    "inventory_timeout": {
      "strategy": "retry_with_backoff",
      "max_retries": 3,
      "backoff_factor": 2,
      "timeout_ms": 5000
    },
    "payment_failure": {
      "strategy": "circuit_breaker",
      "failure_threshold": 3,
      "success_threshold": 2,
      "timeout_ms": 10000
    },
    "shipping_unavailable": {
      "strategy": "fallback",
      "fallback_provider": "backup_shipping",
      "cache_ttl": 300
    }
  },
  "global_config": {
    "max_total_retries": 10,
    "default_timeout_ms": 30000,
    "enable_circuit_breaker": true
  }
}
```

**Estrategias disponibles:**

| Estrategia | Descripci√≥n | Par√°metros |
|------------|-------------|------------|
| `retry_with_backoff` | Retry con exponential backoff | `max_retries`, `backoff_factor` |
| `circuit_breaker` | Circuit breaker pattern | `failure_threshold`, `success_threshold` |
| `fallback` | Fallback a servicio alternativo | `fallback_provider`, `cache_ttl` |
| `compensating_transaction` | Rollback de transacciones | `compensation_steps` |

### Variables de Entorno

**Archivo:** `.env`

```bash
# Google Cloud
GOOGLE_CLOUD_PROJECT=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json

# Vertex AI
VERTEX_AI_LOCATION=us-central1
VERTEX_AI_MODEL=gemini-1.5-flash-002
VERTEX_AI_TEMPERATURE=0.7
VERTEX_AI_MAX_TOKENS=8192

# Chaos Configuration
DEFAULT_FAILURE_RATE=0.3
CHAOS_SEED=42
ENABLE_CHAOS=true

# Experiment Configuration
MAX_CONCURRENT_EXPERIMENTS=5
EXPERIMENT_TIMEOUT=60
RETRY_MAX_ATTEMPTS=3

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/chaos_engine.log

# Results
RESULTS_BASE_DIR=results
KEEP_RAW_RESULTS=true
```

### Configuraci√≥n de Logging

**Archivo:** `src/utils/logger.py`

```python
import logging
from pathlib import Path

def setup_logger(
    name: str = "chaos_engine",
    level: int = logging.INFO,
    log_file: str = "logs/chaos_engine.log"
):
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)
    console_format = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(console_format)
    
    # File handler
    Path(log_file).parent.mkdir(parents=True, exist_ok=True)
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)
    file_format = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    file_handler.setFormatter(file_format)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger
```

---

## üîß Troubleshooting

### Problemas Comunes

#### 1. Error: "Module not found"

**Problema:**
```
ModuleNotFoundError: No module named 'src.core'
```

**Soluci√≥n:**
```powershell
# Asegurar que est√°s en el directorio correcto
cd chaos-playbook-engine

# Reinstalar dependencias
poetry install

# Activar entorno
poetry shell

# Verificar PYTHONPATH
poetry run python -c "import sys; print(sys.path)"
```

#### 2. Error: "Timeout en experimentos"

**Problema:**
```
TimeoutError: Experiment exceeded 60s timeout
```

**Soluci√≥n:**
```powershell
# Aumentar timeout
poetry run python scripts/run_parametric_experiments.py \
    --timeout 120

# O modificar configuraci√≥n por defecto en chaos_config.py
DEFAULT_TIMEOUT = 120
```

#### 3. Error: "Dashboard no genera"

**Problema:**
```
FileNotFoundError: aggregated_metrics.json not found
```

**Soluci√≥n:**
```powershell
# Verificar que experimento complet√≥ correctamente
ls results/parametric_experiments/run_XXXXX/

# Regenerar m√©tricas agregadas
poetry run python -c "
from src.core.aggregate_metrics import aggregate_results
aggregate_results('results/run_XXX/raw_results.csv')
"

# Luego generar dashboard
poetry run python scripts/generate_dashboard.py --latest
```

#### 4. Error: "Tests fallan con fixtures"

**Problema:**
```
fixture 'sample_order_data' not found
```

**Soluci√≥n:**
```powershell
# Verificar que conftest.py est√° en tests/
ls tests/conftest.py

# Ejecutar tests con verbose para ver fixtures
poetry run pytest tests/ -v --fixtures
```

#### 5. Error: "Cobertura baja"

**Problema:**
```
FAILED (coverage < 80%)
```

**Soluci√≥n:**
```powershell
# Ver qu√© l√≠neas faltan cubrir
poetry run pytest tests/ --cov=src --cov-report=term-missing

# Abrir reporte HTML detallado
poetry run pytest tests/ --cov=src --cov-report=html
start htmlcov/index.html

# Agregar tests para m√≥dulos con baja cobertura
```

### Debug Mode

**Habilitar logs detallados:**

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

**En comandos:**

```powershell
# Verbose mode
poetry run python scripts/run_parametric_experiments.py --verbose

# Debug pytest
poetry run pytest tests/ -vv -s --log-cli-level=DEBUG
```

### Limpiar Cach√© y Rebuilds

```powershell
# Limpiar cach√© de Python
find . -type d -name "__pycache__" -exec rm -rf {} +
find . -type f -name "*.pyc" -delete

# Limpiar entorno virtual de Poetry
poetry env remove python
poetry install

# Limpiar resultados temporales
rm -rf results/parametric_experiments/run_*
```

---

## üìö Ap√©ndices

### A. Glosario de T√©rminos

| T√©rmino | Definici√≥n |
|---------|------------|
| **Chaos Engineering** | Pr√°ctica de inyectar fallos controlados para validar resilencia |
| **Playbook** | Conjunto de estrategias de recuperaci√≥n de errores |
| **Baseline Agent** | Agente sin estrategias de recuperaci√≥n |
| **Playbook Agent** | Agente con estrategias de recuperaci√≥n |
| **Failure Rate** | Probabilidad de fallo en APIs (0.0-1.0) |
| **Success Rate** | Porcentaje de experimentos exitosos |
| **Latency Overhead** | Incremento en latencia por estrategias de recuperaci√≥n |
| **Data Consistency** | Porcentaje de datos consistentes entre APIs |
| **A/B Testing** | Comparaci√≥n entre dos variantes de agentes |

### B. Referencias Externas

- **Chaos Engineering**: [principlesofchaos.org](https://principlesofchaos.org/)
- **Poetry Documentation**: [python-poetry.org/docs](https://python-poetry.org/docs/)
- **Pytest Documentation**: [docs.pytest.org](https://docs.pytest.org/)
- **Plotly Documentation**: [plotly.com/python](https://plotly.com/python/)
- **Google Vertex AI**: [cloud.google.com/vertex-ai/docs](https://cloud.google.com/vertex-ai/docs)

### C. Comandos R√°pidos de Referencia

**Setup y Tests:**

```powershell
# Instalaci√≥n inicial
poetry install && poetry shell

# Tests r√°pidos
poetry run pytest tests/ -v

# Tests con cobertura
poetry run pytest tests/ --cov=src --cov-report=html
```

**Experimentos:**

```powershell
# Experimento completo
poetry run python scripts/run_parametric_experiments.py

# Dashboard
poetry run python scripts/generate_dashboard.py --latest
```

**Linting:**

```powershell
# Formatear c√≥digo
poetry run black src/ tests/

# Linter
poetry run flake8 src/ tests/
```

**Logs y Debug:**

```powershell
# Ver logs
tail -f logs/chaos_engine.log

# Debug mode
export LOG_LEVEL=DEBUG
poetry run python scripts/run_parametric_experiments.py --verbose
```

### D. Estructura de Salida de Resultados

```
results/
‚îî‚îÄ‚îÄ parametric_experiments/
    ‚îî‚îÄ‚îÄ run_20251124_003045/
        ‚îú‚îÄ‚îÄ raw_results.csv              # Resultados raw de experimentos
        ‚îú‚îÄ‚îÄ aggregated_metrics.json      # M√©tricas agregadas
        ‚îú‚îÄ‚îÄ dashboard.html               # Dashboard interactivo
        ‚îî‚îÄ‚îÄ experiment.log               # Logs de ejecuci√≥n
```

**Formato de raw_results.csv:**

```csv
experiment_id,agent_type,failure_rate,success,duration_s,inconsistencies,timestamp
1,baseline,0.3,true,2.34,0,2025-11-24T00:30:45
2,baseline,0.3,false,3.12,1,2025-11-24T00:30:48
3,playbook,0.3,true,2.51,0,2025-11-24T00:30:51
...
```

**Formato de aggregated_metrics.json:**

```json
{
  "0.1": {
    "failure_rate": 0.1,
    "n_experiments": 5,
    "baseline": {
      "success_rate": {"mean": 0.8, "std": 0.05, "min": 0.7, "max": 0.9},
      "duration_s": {"mean": 2.1, "std": 0.3, "min": 1.8, "max": 2.5},
      "inconsistencies": {"mean": 0.1, "std": 0.05, "min": 0.0, "max": 0.2}
    },
    "playbook": {
      "success_rate": {"mean": 0.9, "std": 0.03, "min": 0.85, "max": 0.95},
      "duration_s": {"mean": 2.3, "std": 0.2, "min": 2.0, "max": 2.6},
      "inconsistencies": {"mean": 0.05, "std": 0.03, "min": 0.0, "max": 0.1}
    }
  },
  "0.3": {...},
  "0.5": {...}
}
```

---

## üìû Soporte y Contacto

**Desarrollador:** Albert  
**Email:** [your-email@example.com]  
**GitHub:** [github.com/your-username/chaos-playbook-engine]  
**Documentaci√≥n:** [docs.chaos-playbook-engine.com]

---

## üìù Licencia

Este proyecto es parte del **Stanford Google Cloud AI Capstone** y est√° disponible bajo licencia MIT.

---

**√öltima actualizaci√≥n:** 24 Noviembre 2025  
**Versi√≥n del manual:** 2.0