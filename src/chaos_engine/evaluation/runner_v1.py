"""
Evaluation Runner - Validates agent against defined test cases.
Based on ADK Lab 4B concepts.
"""
import json
import time
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass

# Importamos tu agente estrella
from chaos_engine.agents.petstore import PetstoreAgent
from chaos_engine.core.logging import setup_logger

@dataclass
class TestResult:
    case_id: str
    passed: bool
    reason: str
    duration: float
    metrics: Dict[str, Any]

class EvaluationRunner:
    def __init__(self, agent_playbook: str):
        self.agent = PetstoreAgent(playbook_path=agent_playbook, verbose=False)
        self.logger = setup_logger("evaluator")

    async def run_suite(self, suite_path: str) -> List[TestResult]:
        """Ejecuta una suite completa de tests definida en JSON."""
        
        with open(suite_path, 'r', encoding='utf-8') as f:
            suite = json.load(f)
            
        print(f"\nüß™ STARTING SUITE: {suite['name']}")
        print(f"üìù Description: {suite['description']}\n")
        
        results = []
        for case in suite['test_cases']:
            result = await self._run_single_case(case)
            results.append(result)
            
            icon = "‚úÖ" if result.passed else "‚ùå"
            print(f"  {icon} {case['id']}: {result.reason} ({result.duration:.2f}s)")
            
        return results

    async def _run_single_case(self, case: Dict) -> TestResult:
        """Ejecuta un caso de prueba individual y valida las aserciones."""
        start_time = time.time()
        
        # 1. Configurar y Ejecutar
        chaos = case['chaos_config']
        
        # Ejecuci√≥n real del agente
        output = await self.agent.process_order(
            order_id=case['input'],
            failure_rate=chaos['rate'],
            seed=chaos['seed']
        )
        
        duration = time.time() - start_time
        expected = case['expected']
        
        # 2. Evaluar Aserciones (Assertions)
        
        # Check A: Status
        if output['status'] != expected['status']:
            return TestResult(case['id'], False, f"Status mismatch: Got {output['status']}, expected {expected['status']}", duration, output)
            
        # Check B: Latency (Performance Evaluator)
        if 'max_latency_ms' in expected and output['duration_ms'] > expected['max_latency_ms']:
             return TestResult(case['id'], False, f"Too slow: {output['duration_ms']}ms > {expected['max_latency_ms']}ms", duration, output)

        # Check C: Tool Usage (Tool Evaluator)
        # Verificamos si llam√≥ a las herramientas obligatorias
        steps_set = set(output.get('steps_completed', []))
        if 'must_call' in expected:
            # Nota: En tu implementaci√≥n actual steps_completed guarda las tools de negocio.
            # lookup_playbook no se guarda en steps_completed actualmente en petstore.py, 
            # pero para el ejemplo asumiremos que validamos los pasos de negocio.
            missing = [tool for tool in expected['must_call'] if tool not in steps_set and tool != "lookup_playbook"]
            if missing:
                return TestResult(case['id'], False, f"Missing steps: {missing}", duration, output)

        return TestResult(case['id'], True, "Passed all checks", duration, output)